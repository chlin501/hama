/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hama.client

import akka.actor.ActorRef
import akka.actor.ActorSystem
import akka.actor.Props
import java.io.DataOutputStream
import java.io.IOException
import org.apache.hadoop.fs.Path
import org.apache.hadoop.io.WritableUtils
import org.apache.hama.Event
import org.apache.hama.EventListener
import org.apache.hama.HamaConfiguration
import org.apache.hama.ProxyInfo
import org.apache.hama.RemoteService
import org.apache.hama.bsp.BSPJob
import org.apache.hama.bsp.BSPJobClient
import org.apache.hama.bsp.BSPJobID
import org.apache.hama.bsp.FileSplit
import org.apache.hama.conf.Setting
import org.apache.hama.io.PartitionedSplit
import org.apache.hama.fs.Operation
import org.apache.hama.master.Reject
import org.apache.hama.master.Submit
import org.apache.hama.util.MasterDiscovery
import scala.util.Failure
import scala.util.Random
import scala.util.Success
import scala.util.Try

final case object JobCompleteEvent extends Event

trait SubmitterMessage
final case object Request extends SubmitterMessage
final case class JobComplete(jobId: BSPJobID) extends SubmitterMessage
final case class WorkingDirs(jobDir: Path, splitPath: Path, jarPath: Path,
                             jobPath: Path) extends SubmitterMessage
final case class Response(id: BSPJobID, sysDir: Path, maxTasks: Int) 
      extends SubmitterMessage
final case class SubmitJob(job: BSPJob) extends SubmitterMessage

object Submitter {

  val submit_ = "submit_"

  private var submitter: Option[ActorRef] = None

  def startup(args: Array[String]) {
    val setting = Setting.client
    val system = ActorSystem(setting.info.getActorSystemName, setting.config) 
    submitter = Option(system.actorOf(Props(setting.main, setting), 
                       setting.name))
  }

  def simpleName(conf: HamaConfiguration): String = 
    conf.get("client.name", classOf[Submitter].getSimpleName) + "#" +
    Random.nextInt  

  /**
   * Submit an assembled bsp job through submitter before actually submitting
   * to master.
   * @param job is an BSPJob containing related configuration and code to be
   *            executed.
   */
  def submit(job: BSPJob): Boolean = submitter.map { client => 
    client ! SubmitJob(job)
    true
  }.getOrElse(false)

}

class Submitter(setting: Setting) extends RemoteService with MasterDiscovery 
                                                        with EventListener {

  import Submitter._
  import Operation._
  import PartitionedSplit._

  protected val rand = new Random

  protected var masterProxy: Option[ActorRef] = None

  protected var bspJob: Option[BSPJob] = None

  /* id generated by master. */
  protected var bspJobId: Option[BSPJobID] = None

  /* master's system directory. */
  protected var sysDir: Option[Path] = None

  /* max tasks allowed found for the client. */
  protected var maxTasksAllowed: Int = 1 

  override def setting(): Setting = setting

  override def initializeServices = retry("discover", 10, discover)

  override def afterLinked(target: String, proxy: ActorRef): Unit = {
    masterProxy = Option(proxy)
    bspJob.map { job => askForMetadata }
  }

  /**
   * Only bspJob is cached. 
   * Note sender is not cached because it's deadLetters!
   * @param job is a bsp job.
   */
  protected def cache(job: BSPJob) = bspJob = Option(job)

  protected def askForMetadata() = bspJobId match {
    case Some(id) => LOG.warning("Job id alreay exists: {}!", id)
    case None => masterProxy.map { m => m ! Request }
  }

  protected def clientMsg: Receive = {
    case SubmitJob(job) => masterProxy match {
      case Some(m) => { askForMetadata; cache(job) }
      case None => cache(job)
    }
  }

  protected def settingForClient(id: BSPJobID, dir: Path, maxTasks: Int) {
    bspJobId = Option(id)
    sysDir = Option(dir)
    maxTasksAllowed = maxTasks
  }
  
  protected def masterMsg: Receive = {
    /**
     * Master creates a new job id for unsubmitted bsp job; and master's system
     * directory path.
     */
    case Response(id, dir, maxTasks) => {
      settingForClient(id, dir, maxTasks)
      bspJob match { 
        case Some(job) => Try(submitInternal(job, id, dir, maxTasks)) match {
          case Success(result) =>
          case Failure(cause) => { cleanup; shutdown }
        }
        case None => LOG.error("Unlikely but no bsp job submitted!")
      }
    }
    /**
     * Scheduler replies directly when the job is finished.
     */    
    case JobComplete(jobId) => { cleanup; shutdown }
    case failure: Reject => { 
      LOG.error("Failing processing job because {}", failure.reason)
      cleanup; shutdown
    }
  }

  // TODO: BSPJobClient.submitJobInternal
  protected def submitInternal(job: BSPJob, id: BSPJobID, sysDir: Path, 
                               maxTasks: Int) {
    job.setJobID(id) 
    val dirs = workingDirs(sysDir) 
    val splitPath = dirs.splitPath
    val adjustedJob = adjustTasks(job, maxTasks)
    (adjustedJob.hasInputPath || adjustedJob.hasJoinExpr) match {
      case true => {
        val splits = defaultSplit(dirs, adjustedJob, maxTasks)
        val out = writeSplitsHeader(job.configuration, splitPath, splits.length)
        val numSplits = writeSplits(adjustedJob, splits, out)
        adjustedJob.setNumBspTask(numSplits)
        adjustedJob.set("bsp.job.split.file", splitPath.toString)
      }
      case false => 
    }
    val operation = Operation.get(setting.hama)
    adjustedJob.getJar match {
      case null => LOG.warning("Jar path is not set! User classes may not be " +
                               "found. Set BSPJob#setJar(String) or check "+
                               "your jar file.")
      case originalJarPath@_ => {
        adjustedJob.setJobNameIfEmpty(originalJarPath)
        adjustedJob.setJar(splitPath.toString)
        operation.copyFromLocal(new Path(originalJarPath))(splitPath)
        operation.setReplication(splitPath, adjustedJob.replication) 
        operation.setPermission(splitPath, newPermission(jobFilePermission))
      }
    }
    adjustedJob.setUser(BSPJobClient.getUnixUser)
    adjustedJob.setGroupBy(BSPJobClient.getUnixGroupBy(adjustedJob.getUser))
    adjustedJob.setWorkingDirectoryIfEmpty(operation.getWorkingDirectory)

    operation.create(splitPath, newPermission(jobFilePermission)) match {
      case null => { 
        LOG.error("Can't create path at {}", splitPath)
        cleanup
        shutdown 
      }
      case out@_ => try { job.writeXml(out) } finally { out.close }
    }
    masterProxy.map { m => bspJobId.map { id => {
      val operation = owns(sysDir, setting.hama)
      m ! Submit(id, operation.makeQualified(splitPath).toString)
    }}}

  }

  protected def writeSplits(job: BSPJob, splits: Array[PartitionedSplit],
                            out: DataOutputStream): Int = try {
    splits.map { split => { split.write(out); split }}.length
  } finally { out.close }

  /**
   * Create an output stream with header information added.
   * @param conf is a job configration.
   * @param splitPath is the dest path for split files.
   * @param splitSize is the splits array created.
   */
  protected def writeSplitsHeader(conf: HamaConfiguration,
                                  splitPath: Path,
                                  splitSize: Int): DataOutputStream = {
    val operation = owns(splitPath, conf)
    val out = toDataOutputStream(operation.create(splitPath, jobFilePermission))
    out.write(splitFileHeader)
    WritableUtils.writeVInt(out, currentSplitFileVersion)
    WritableUtils.writeVInt(out, splitSize)
    out
  }

  protected def adjustTasks(job: BSPJob, maxTasksAllowed: Int): BSPJob =
    (maxTasksAllowed < job.getNumBspTask) match {
      case true => { job.setNumBspTask(maxTasksAllowed); job }
      case false => job
    }

  protected def defaultSplit(workingDirs: WorkingDirs, 
                             job: BSPJob, 
                             maxTasks: Int): Array[PartitionedSplit] = 
    splitsForJob(workingDirs, job, maxTasks, partition)

  protected def splitsForJob(workingDirs: WorkingDirs, 
                             job: BSPJob, 
                             maxTasks: Int, 
                             partition: (BSPJob) => BSPJob): 
      Array[PartitionedSplit] = {
    // Note: original bsp job client set replication to 10
    //val replication = job.replication 
    val splits = job.getInputFormat.getSplits(job, maxTasks)
    val partitioned = partition(job)
    val newMaxTasks = partitioned.getInt("hama.partition.count", maxTasks)
    val newSplits = partitioned.hasPartitioned match {
      case true => partitioned.getInputFormat.getSplits(partitioned, maxTasks) 
      case false => Array[PartitionedSplit]()
    }
    if(newMaxTasks < newSplits.length) 
      throw new IOException("The number of splits "+newSplits.length+
                            " exceeds that of max tasks "+newMaxTasks)
    newSplits.map { split => split.isInstanceOf[FileSplit] match {
      case true => PartitionedSplit.from(split.asInstanceOf[FileSplit]) 
      case false => throw new RuntimeException("Split is not convertable!")
    }}
  }

  /**
   * Submit job to master for partitioning data.
   */
  // TODO: submit job to master for runtime partitioning
  protected def partition(job: BSPJob): BSPJob = job 

  /**
   * Formulate paths to be used, including 
   * - the path in which split files are saved.  
   * - the path the jar file is stored.
   * - the path where the job xml is stored.
   */
  protected def workingDirs(sysDir: Path): WorkingDirs = { 
    val randomValue = Integer.toString(Math.abs(rand.nextInt), 36)
    val jobDir = new Path(sysDir, "%s%s".format(submit_, randomValue))
    val splitPath = new Path(jobDir, "job.split") 
    val jarPath = new Path(jobDir, "job.jar") 
    val jobPath = new Path(jobDir, "job.xml") 

    val operation = owns(sysDir, setting.hama)
    operation.remove(jobDir)
    val newJobDir = new Path(new Path(operation.makeQualified(jobDir)).
                                      toUri.getPath)
    operation.mkdirs(newJobDir)
    WorkingDirs(newJobDir, splitPath, jarPath, jobPath)
  }

  /**
   * Cleanup job related information hold by this client.
   */
  protected def cleanup() = {
    LOG.info("Cleanup job related information and then shutdown sytem ...")
    bspJobId = None 
    bspJob = None
    masterProxy = None
  }

  // TODO: master periodically reports progress
  //protected def process: Receive = { 
    //case Progress => 
  //}

  override def receive = clientMsg orElse masterMsg orElse unknown

}
